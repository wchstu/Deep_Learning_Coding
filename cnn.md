# å·ç§¯ç¥ç»ç½‘ç»œå…¬å¼æ¨å¯¼åŠnumpyå®ç°

æœ¬æ–‡ä¸»è¦ä¾§é‡äºç½‘ç»œçš„ä»£ç å®ç°ï¼Œå…·ä½“çš„å…¬å¼æ¨å¯¼å¯å‚è€ƒï¼šhttps://zhuanlan.zhihu.com/p/61898234

å®Œæ•´ä»£ç ï¼šhttps://github.com/hui126/Deep_Learning_Coding/blob/main/Conv.py

å·ç§¯ç¥ç»ç½‘ç»œå¯ä»¥çœ‹ä½œæ˜¯æ„ŸçŸ¥æœºç½‘ç»œçš„æ‹“å±•ï¼Œç¥ç»å…ƒçš„æ•°ç›®ç­‰äºå›¾åƒçš„é€šé“æ•°ï¼Œè¾“å…¥åˆ°ç½‘ç»œä¸­çš„å€¼ç”±å‘é‡å˜ä¸ºå¼ é‡ï¼Œä¸æ„ŸçŸ¥æœºç½‘ç»œæœ€å¤§çš„ä¸åŒåœ¨äºä½¿ç”¨æƒå€¼å…±äº«ï¼Œå³æ¯ä¸€é€šé“å·ç§¯è¿ç®—è¿‡ç¨‹ä¸­å…±äº«ä½¿ç”¨ä¸€ä¸ªå·ç§¯æ ¸ã€‚

## å‰å‘ä¼ é€’

åŸºäºnumpyï¼Œå‡è®¾è¾“å…¥ç‰¹å¾å›¾$a$ç»´åº¦ä¸º$(1,3,4,4)$ï¼Œå·ç§¯æ ¸$w$çš„ç»´åº¦ä¸º$(2, 3, 2, 2)$ï¼Œæ­¥é•¿ä¸º$(1, 1)$ï¼Œ
$$
a[0,0,:,:] = \left[\begin{matrix}
1 & 6 & 6 & 2 \\ 4 & 3 & 4 & 3 \\ 3 & 6 & 7 & 7 \\ 1 & 5 & 1 & 2
\end{matrix}\right], a[0,1,:,:] = \left[\begin{matrix}
9 & 1 & 6 & 7 \\ 5 & 3 & 2 & 6 \\ 5 & 7 & 1 & 7 \\ 6 & 8 & 5 & 8
\end{matrix}\right], a[0,2,:,:] = \left[\begin{matrix}
8 & 6& 9 & 5 \\ 4 & 6 & 1 & 6 \\ 2 & 3 & 3 & 8 \\ 3 & 5 & 3 & 6
\end{matrix}\right]
$$
å·ç§¯æ ¸ä¸º
$$
w[0,0,:,:] = \left[\begin{matrix} 5 & 9 \\ 5 & 8\end{matrix}\right],
w[0,1,:,:] = \left[\begin{matrix} 1 & 1 \\ 8 & 8\end{matrix}\right], w[0,2,:,:] = \left[\begin{matrix} 7 & 1 \\ 2 & 8\end{matrix}\right] \\
w[1,0,:,:] = \left[\begin{matrix} 5 & 6 \\ 9 & 3\end{matrix}\right],
w[1,1,:,:] = \left[\begin{matrix} 2 & 1 \\ 9 & 1\end{matrix}\right], w[1,2,:,:] = \left[\begin{matrix} 8 & 4 \\ 3 & 6\end{matrix}\right]
$$
åç½®ç³»æ•°ä¸º$b=[1, 2]$

åˆ™è¾“å‡ºç‰¹å¾å›¾ä¸º
$$
z[i,j,:,:] = \sum^{2}_{k=0}a[i,k,:,:]*w[j,k,:,:] + b[j]
$$
ç»“æœä¸ºï¼š
$$
z[0,0,:,:] = \left[\begin{matrix}
296 & 250 & 288 \\ 277 & 280 & 294\\ 302 & 297 & 315
\end{matrix}\right], z[0,1,:,:] = \left[\begin{matrix}
291 & 252 & 263 \\ 230 & 267 & 239 \\ 223 & 283 & 257
\end{matrix}\right]
$$
ä¸ºäº†ä¾¿äºæ¢¯åº¦åå‘ä¼ æ’­è®¡ç®—ï¼Œæˆ‘ä»¬å°†å¯¹å·ç§¯æ ¸ä¸è¾“å…¥ç‰¹å¾å›¾è¿›è¡Œå˜æ¢ï¼Œå°†å·ç§¯è¿ç®—è½¬åŒ–ä¸ºçŸ©é˜µä¹˜æ³•è¿ç®—ï¼Œå…¶ä¸­å·ç§¯æ ¸è½¬ä¸º$(3*2*2,2)$çš„çŸ©é˜µï¼Œ
$$
w_t = w.reshape(-1,2).T= \left[\begin{matrix}
5&9&5&8&1&1&8&8&7&1&2&8\\ 5&6&9&3&2&1&9&1&8&4&3&6
\end{matrix}\right]^T
$$
è¾“å…¥ç‰¹å¾å›¾è½¬ä¸º$(1,9,12)$çš„çŸ©é˜µï¼Œ
$$
a_t[0]=\left[\begin{matrix}
1& 6& 4& 3& 9& 1& 5& 3& 8& 6& 4& 6 \\
6& 6& 3& 4& 1& 6& 3& 2& 6& 9& 6& 1\\
6& 2& 4& 3& 6& 7& 2& 6& 9& 5& 1& 6\\
4& 3& 3& 6& 5& 3& 5& 7& 4& 6& 2& 3\\
3& 4& 6& 7& 3& 2& 7& 1& 6& 1& 3& 3\\
4& 3& 7& 7& 2& 6& 1& 7& 1& 6& 3& 8\\
3& 6& 1& 5& 5& 7& 6& 8& 2& 3& 3& 5\\
6& 7& 5& 1& 7& 1& 8& 5& 3& 3& 5& 3\\
7& 7& 1& 2& 1& 7& 5& 8& 3& 8& 3& 6
\end{matrix}\right]\\
$$
åˆ™$z_t[0] = a_t[0]w_t$ï¼Œå…¶ä¸­$z_t[0]$çš„æ¯ä¸€åˆ—å¯¹åº”äºå·ç§¯è¾“å‡ºçš„ç¬¬ä¸€å¼ ç‰¹å¾å›¾çš„æ¯ä¸€ä¸ªé€šé“çš„å€¼ï¼Œæ‰€ä»¥$z=z_t.transpose([0, 2, 1]).rehsape(1,2,3,3)+b$

---

ç¬¦å·çº¦å®šï¼šå‡è®¾æ¯ä¸€å±‚çš„ç¥ç»å…ƒæ•°ç›®ï¼ˆè¾“å‡ºç‰¹å¾å›¾é€šé“æ•°ï¼‰ä¸º$n^l$ï¼Œå…±$L$å±‚ï¼Œå…¶ä¸­$n^0$ä¸ºè¾“å…¥å›¾åƒçš„é€šé“æ•°ç›®ã€‚

$a^{l-1}$	ç¬¬$l$å±‚å·ç§¯å±‚çš„è¾“å…¥ç‰¹å¾å›¾ï¼Œç»´åº¦ä¸º$(B,C^{l-1},H^{l-1},W^{l-1})$ï¼›

$z^l$	    å·ç§¯è¾“å‡ºç»“æœï¼Œæœªç»è¿‡æ¿€æ´»å‡½æ•°ï¼Œç»´åº¦ä¸º$(B,C^l,H^l,W^l)$

$h(z)$	æ¿€æ´»å‡½æ•°ï¼›

$w^l$	   å·ç§¯æ ¸ï¼Œç»´åº¦ä¸º$(C^{l}, C^{l-1}, h^l, w^l)$ï¼›

$b^l$ 	   åç½®ç³»æ•°ï¼Œç»´åº¦ä¸º$(C^l,)$

å·ç§¯è¾“å‡ºç»“æœä¸ºï¼Œ
$$
z^{l}[i,j,:,:] = \sum^{C^{l-1}-1}_{k=0}a^{l-1}[i,k,:,:]*w^l[j,k,:,:] + b^l[j] \quad i=0,\cdots,B-1,j=0,\cdots,C^l-1
$$
å°†å·ç§¯å¤„ç†è¿‡ç¨‹è½¬åŒ–ä¸ºçŸ©é˜µä¹˜ç§¯ï¼Œ
$$
a_t^{l-1} = trans(a^{l-1}), dim=(B, H^l\cdot W^l,h^l\cdot w^l\cdot C^{l-1})
$$
å…¶ä¸­$trans(a^{l-1})$ä¸ºå°†æ¯ä¸€ä¸ªå·ç§¯æ ¸åœç•™å¤„çš„å¯¹åº”æ•°å€¼å±•æˆä¸€è¡Œï¼Œå­˜å‚¨åœ¨$a^{l-1}_t$ä¸­ã€‚
$$
w_t = w.reshape(-1, C^l).T
$$

$$
z_t^l = a^{l-1}_tw_t \\ 
z^l = z_t^l.transpose([0, 2, 1]).reshape(B, C^l, H^l,W^l)+b.reshape(1, -1, 1, 1)
$$

```python
def forward(self, inputs):
    inputs = self.pad(inputs)
    self.input_shape = inputs.shape
    self.batch_size, in_channels, self.H_in, self.W_in = inputs.shape
    assert in_channels == self.in_channels, 'inputs dim1({}) is not equal to convolutional in_channels({})'.format(in_channels, self.in_channels)

    self.H_out = (inputs.shape[2] - self.kernel_size[0]) // self.stride[0] + 1
    self.W_out = (inputs.shape[3] - self.kernel_size[1]) // self.stride[1] + 1

    self.input_trans = np.empty((self.batch_size, self.H_out * self.W_out, self.kernel_trans.shape[0]))

    ind = 0
    h = 0
    while (h + self.kernel_size[0] <= inputs.shape[2]):
        w = 0
        while (w + self.kernel_size[1] <= inputs.shape[3]):
            self.input_trans[:, ind, :] = inputs[:, :, h:h + self.kernel_size[0], w:w + self.kernel_size[1]].reshape(self.batch_size, -1)
            w += self.stride[1]
            ind += 1
            h += self.stride[0]

            output = self.input_trans @ self.kernel_trans
            output = output.transpose([0, 2, 1]).reshape(self.batch_size, self.out_channels, self.H_out, self.W_out)
            if self.bias is not None:
                output += self.bias.reshape(1, -1, 1, 1)
	return self.input_trans, output
```

## åå‘ä¼ æ’­

ä¸å…¨è¿æ¥å±‚ç±»ä¼¼ï¼Œåœ¨è¿›è¡Œæ¢¯åº¦åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œè®¡ç®—æŸå¤±å‡½æ•°å¯¹$z^l$çš„åå‘ä¼ æ’­è¯¯å·®ï¼Œç„¶åå†è®¡ç®—å¯¹å·ç§¯æ ¸åŠåç½®çš„å¯¼æ•°ã€‚

å‡è®¾è¾“å…¥ç‰¹å¾å›¾ä¸º
$$
a = \left[\begin{matrix} a_{11}&a_{12}&a_{13}&a_{14} \\a_{21}&a_{22}&a_{23}&a_{24} \\a_{31}&a_{32}&a_{33}&a_{34}\\a_{41}&a_{42}&a_{43}&a_{44}\end{matrix}\right]
$$
å·ç§¯æ ¸ä¸º
$$
w = \left[\begin{matrix}w_{11}&w_{12}\\w_{21}&w_{22}\end{matrix}\right]
$$
å·ç§¯æ­¥é•¿ä¸º$(1,1)$ï¼ŒæŸå¤±å‡½æ•°å¯¹å·ç§¯ç»“æœçš„åå‘ä¼ æ’­è¯¯å·®ä¸ºï¼š
$$
\delta = \left[\begin{matrix} \delta_{11}&\delta_{12}&\delta_{13} \\ \delta_{21}&\delta_{22}&\delta_{23} \\\delta_{31}&\delta_{32}&\delta_{33} \end{matrix}\right]
$$
åˆ™æŸå¤±å‡½æ•°å¯¹è¾“å…¥ç‰¹å¾å›¾çš„æ¢¯åº¦ä¸ºï¼š
$$
\left[\begin{matrix}
w_{11}\delta_{11} & w_{11}\delta_{12}+w_{12}\delta_{11}& w_{12}\delta_{12}+w_{11}\delta_{13}& w_{12}\delta_{13} \\
w_{21}\delta_{11}+w_{11}\delta_{21}& w_{22}\delta_{11}+w_{21}\delta_{12}+w_{12}\delta_{21}+w_{11}\delta_{22}& w_{22}\delta_{12}+w_{21}\delta_{13}+w_{12}\delta_{22}+w_{11}\delta_{23}&
w_{21}\delta_{13}+w_{11}\delta_{23} \\
w_{21}\delta_{21}+w_{11}\delta_{31}& w_{22}\delta_{21}+w_{21}\delta_{22}+w_{12}\delta_{31}+w_{11}\delta_{32}& w_{22}\delta_{22}+w_{21}\delta_{23}+w_{12}\delta_{32}+w_{11}\delta_{33}&
w_{21}\delta_{23}+w_{11}\delta_{33} \\
w_{21}\delta_{31} & w_{22}\delta_{31}+w_{21}\delta_{32}& w_{22}\delta_{32}+w_{21}\delta_{33}& w_{22}\delta_{33}
\end{matrix}\right]
$$
å³å°†å¯¹è¾“å‡ºå›¾çš„è¯¯å·®è¿›è¡Œ0å¡«å……åï¼Œä¸å·ç§¯æ ¸æ—‹è½¬180åº¦åè¿›è¡Œå·ç§¯ï¼Œå³è·å¾—æŸå¤±å‡½æ•°å¯¹è¾“å…¥ç‰¹å¾å›¾çš„è¯¯å·®ã€‚

æ‰€ä»¥å½“å·²çŸ¥$\delta^{l+1}$æ—¶ï¼Œè®¡ç®—$\delta^l$ï¼Œ
$$
\delta^l = \delta^{l+1}*ROT180(w^{l+1})\odot \frac{\partial a^l}{\partial z^l}
$$

---

åœ¨è¿™é‡ŒåŒæ ·æ¢ä¸€ç§æ€è·¯ï¼Œæ ¹æ®ä¸Šä¸€å°èŠ‚çš„è½¬æ¢åçš„å‰å‘ä¼ æ’­å…¬å¼ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æŸå¤±å‡½æ•°å¯¹$a^{l-1}_t$çš„åå‘ä¼ æ’­è¯¯å·®ï¼Œç„¶åå°†ç»“æœè½¬ä¸ºå¯¹åº”çš„$a^{l-1}$ã€‚å·²çŸ¥$\delta^l$ï¼Œ
$$
\delta^l_t = \delta^l.transpose([0, 2, 3, 1]).reshape(B, H^l\cdot W^l,C^l)\\
$$
è®¡ç®—$\delta^l_t,dim=(B,H^lW^l,C^l)$ä¸$(w^l_t)^T,dim=(C^l,C^{l-1}h^lw^l)$çš„å¼ é‡ä¹˜ç§¯å³å¯è·å¾—æŸå¤±å‡½æ•°å¯¹$a^{l-1}_t$çš„æ¢¯åº¦ä¿¡æ¯ã€‚
$$
\frac{\partial C}{\partial a^{l-1}_t} = np.tensordot(\delta^l_t,(w^l_t)^T, [(2),(0)])
$$
å…¶ä¸­$[(2),(0)]$è¡¨ç¤ºå¯¹$\delta^l_t$çš„ç¬¬3ç»´åº¦å’Œ$(w^l_t)^T$çš„ç¬¬1ç»´åº¦è¿›è¡Œè®¡ç®—ï¼Œç»“æœçš„ç»´åº¦ä¸º$(B,H^l,W^l, C^{l-1}h^lw^l)$ã€‚

å°†è·å¾—ä¸­é—´è¯¯å·®ä¿¡æ¯åå‘å˜æ¢ï¼ˆæ˜ å°„åˆ°åŒä¸€ä½ç½®å¤„æ‰§è¡ŒåŠ æ³•è¿ç®—ï¼‰å¯ä»¥è·å¾—æŸå¤±å‡½æ•°å¯¹$a^{l-1}$çš„æ¢¯åº¦ã€‚

```python
def backward(self, grad):
	grad_trans = grad.transpose([0, 2, 3, 1]).reshape(self.batch_size, -1, self.out_channels)
	grad_backward_trans = np.tensordot(grad_trans, self.kernel_trans.T, [(2), [0]])
	grad_backward = np.zeros(self.input_shape)

	ind = 0
	for ih in range(grad.shape[2]):
		begin_h = ih * self.stride[0]
		for iw in range(grad.shape[3]):
			begin_w = iw * self.stride[1]
			grad_backward[:, :, begin_h:(begin_h+self.kernel_size[0]), begin_w:(begin_w+self.kernel_size[1])] += \
			grad_backward_trans[:, ind, :].reshape(self.batch_size, self.in_channels, self.kernel_size[0], self.kernel_size[1])
			ind += 1
	grad_backward = grad_backward[:, :, self.padding[0]:self.input_shape[2]-self.padding[0], self.padding[1]:self.input_shape[3]-self.padding[1]]
	# print(grad_backward.shape)

	self.grad_k_trans = np.tensordot(self.input_trans, grad_trans, [(0, 1), (0, 1)])
	if self.bias is not None:
		self.grad_b = np.sum(grad_trans, axis=(0, 1)).reshape(1, -1)
	return grad_backward
```

å·²çŸ¥$\delta^l_t$æ—¶ï¼Œè®¡ç®—æŸå¤±å‡½æ•°å¯¹$w^l_t,b^l$çš„æ¢¯åº¦ï¼Œ
$$
\frac{\partial C}{\partial w^l_t} = np.tensordot(a^{l-1}_t,\delta^l_t,[(0,1),(0,1)]) \\
\frac{\partial C}{\partial b^l} = np.sum(\delta^l_t, axis=(0,1))
$$
å¯¹äºæœ€å¤§æ± åŒ–ï¼Œå¯ä»¥åˆ©ç”¨ç›¸è¿‘çš„æ€æƒ³è¿›è¡Œå¤„ç†ã€‚

å‡è®¾è¾“å…¥ç‰¹å¾å›¾$a$ç»´åº¦ä¸º$(16, 3, 128, 128)$ï¼Œå·ç§¯æ ¸$w$ä¸º$(8, 3, 3, 3)$ï¼Œåç½®$b$ä¸º$(8)$æ­¥é•¿ä¸º$(1,1)$ï¼Œä¸è¿›è¡Œå¡«å……ï¼Œåˆ™å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸ºï¼š

<img src='./images/forward.png'>

è·å¾—è¾“å‡ºç‰¹å¾å›¾çš„åå‘ä¼ æ’­è¯¯å·®$\delta(16, 8, 127,127)$åï¼Œè®¡ç®—å¯¹è¾“å…¥ç‰¹å¾å›¾çš„åå‘ä¼ æ’­è¯¯å·®ï¼Œ
$$
ğ›¿(16,3,127,127)â†’ğ›¿_ğ‘¡ (16,16129, 8)â†’ğ›¿_ğ‘¡^â€²=ğ›¿_ğ‘¡ ğ‘¤_ğ‘¡^ğ‘‡,(16,16129,27)â†’ğ›¿â€²(16,3,128,128)
$$
å¯¹mnistæ•°æ®é›†è¿›è¡Œåˆ†ç±»ï¼Œæ„å»ºç½‘ç»œç»“æ„å¦‚ä¸‹ï¼š

```python
layers = [Conv2d(in_channels=1, out_channels=6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)),
          MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),
          ReLU(),
          Conv2d(in_channels=6, out_channels=16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)),
          MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),
          ReLU(),
          Flatten(),
          Linear(in_features=784, out_features=120),
          ReLU(),
          Linear(in_features=120, out_features=10)]
```

ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™è¿›è¡Œæ¢¯åº¦æ›´æ–°ï¼Œå®Œæˆ5è½®è®­ç»ƒï¼Œè®­ç»ƒæŸå¤±å˜åŒ–æ›²çº¿ä¸º

<img src='./images/cnn_loss.png' style='zoom:0.7'>

éªŒè¯å‡†ç¡®ç‡å˜åŒ–æ›²çº¿ä¸ºï¼š

<img src='./images/cnn_acc.png' style='zoom:0.7'>

æµ‹è¯•é›†å‡†ç¡®ç‡ä¸º0.9798ã€‚

> githubï¼šhttps://github.com/hui126/Deep_Learning_Coding
